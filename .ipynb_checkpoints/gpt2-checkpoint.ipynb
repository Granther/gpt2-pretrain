{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff2aba62-7f49-4dfb-be6c-28c966aed4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import warnings\n",
    "import math\n",
    "\n",
    "#from hf\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, GPT2Model\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a89c0a9f-66bc-42c0-9c28-8a23aaa43f3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7f32efb79204429ba08aca32fd5a5e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cb288ecd89d468d944cbf10543c54b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2000000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7216460d116b4bcc8efaf0f3c20f69a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/2000000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db3ae6bed6494cb5b2d1f47e3e3b0203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f1cb40d2cb64ccfadc0ebb3f38ac934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/14450 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 14450\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading raw data\n",
    "dataset = load_dataset(\"raddwolf/BookCorpus74M\",trust_remote_code=True)\n",
    "\n",
    "# make splits\n",
    "dataset = dataset['train'].select(range(2000000))\n",
    "\n",
    "dataset.train_test_split(test_size=0.0015) \n",
    "\n",
    "# load the gpt-2 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token=tokenizer.eos_token\n",
    "\n",
    "# tokenize\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(text=example[\"text\"])\n",
    "tokenized_ds = dataset.map(tokenize_function,batched=True,remove_columns='text')\n",
    "\n",
    "# save to disk if required (use load_from_disk latter)\n",
    "tokenized_ds.save_to_disk('bookcorpus/tokenized_ds')\n",
    "\n",
    "# Make samples to a size of 1024\n",
    "def concat(examples):    \n",
    "    examples[\"input_ids\"]=[list(chain.from_iterable(examples['input_ids']))] # convert chain to list of tokens\n",
    "    examples[\"attention_mask\"]=[list(chain.from_iterable(examples['attention_mask']))] # convert chain to list of tokens\n",
    "    return examples\n",
    "    \n",
    "# takes a lot of time (worth saving it to disk)\n",
    "concated_ds = tokenized_ds.map(concat,batched=True,batch_size=1000000,num_proc=8)\n",
    "\n",
    "def chunk(examples):\n",
    "    chunk_size = 1024 # modify this accordingly       \n",
    "    input_ids = examples[\"input_ids\"][0] # List[List], pass the inner list      \n",
    "    attention_mask = examples[\"attention_mask\"][0] # List[List]\n",
    "    input_ids_truncated = []\n",
    "    attention_mask_truncated = []\n",
    "    \n",
    "    #slice with step_size=chunk_size\n",
    "    for i in range(0,len(input_ids),chunk_size):\n",
    "        chunk = input_ids[i:i+chunk_size]\n",
    "        if len(chunk)==chunk_size: # drop the last chunk if not equal\n",
    "            input_ids_truncated.append(chunk)\n",
    "            attention_mask_truncated.append(attention_mask[i:i+chunk_size])     \n",
    "    examples['input_ids']=input_ids_truncated\n",
    "    examples[\"attention_mask\"]=attention_mask_truncated\n",
    "        \n",
    "    return examples   \n",
    "\n",
    "chunked_ds = concated_ds.map(chunk,batched=True,batch_size=2,num_proc=2)\n",
    "chunked_ds.save_to_disk('bookcorpus/chunked_ds') # will use this latter for diff experimentation\n",
    "\n",
    "chunked_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04403d8d-ac41-4380-b954-82e080c1f727",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer,mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b5046f0-800a-4763-8ecd-0071e23d3a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33,888,384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2521646425.py:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(model=model,\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "configuration = GPT2Config(\n",
    "    n_head=6,\n",
    "    n_embd=384,\n",
    "    n_layer=8,\n",
    ")\n",
    "model = GPT2LMHeadModel(configuration)\n",
    "model.to(\"cuda\")\n",
    "# model = GPT2Model(configuration)\n",
    "print(f\"{model.num_parameters():,}\")\n",
    "\n",
    "# training arguments\n",
    "training_args = TrainingArguments( output_dir='gpt-2-warm-up/standard-gpt',\n",
    "                                  # evaluation_strategy=\"steps\",\n",
    "                                  # eval_steps=500,                                  \n",
    "                                  num_train_epochs=1,\n",
    "                                  per_device_train_batch_size=4,\n",
    "                                  # per_device_eval_batch_size=8,\n",
    "                                  learning_rate=2.5e-4,\n",
    "                                  lr_scheduler_type='cosine',\n",
    "                                  warmup_ratio=0.05,\n",
    "                                  adam_beta1=0.9,\n",
    "                                  adam_beta2=0.999,                                  \n",
    "                                  weight_decay=0.01,                                  \n",
    "                                  logging_strategy=\"steps\",\n",
    "                                  logging_steps = 50,\n",
    "                                  save_steps=10,\n",
    "                                  save_total_limit=10,                                  \n",
    "                                 ) \n",
    "trainer = Trainer(model=model,\n",
    "                 args = training_args,\n",
    "                 tokenizer=tokenizer,\n",
    "                 train_dataset=chunked_ds,\n",
    "                 # eval_dataset=chunked_ds,\n",
    "                 data_collator = data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f41ec51-623d-437a-8e72-92bcb5eddeb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n",
      "/home/grant/.venv/lib/python3.11/site-packages/transformers/trainer.py:3354: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "/home/grant/.venv/lib/python3.11/site-packages/transformers/trainer.py:3033: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2009' max='3613' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2009/3613 : < :, Epoch 0.56/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30364ffd-3b81-4057-bc59-d65a5916d622",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"I was going to kill the dog at the man in the house , but it , but the man .she 's voice and her eyes .she did n't think she 'd be the first thing she was n't know that she 'd done this , but the other men she was n't going to do anything to be the same .she would have the man .and she was so she 'd have to be a man she did .he wanted to the other way .she 'd thought of a little .she 'd\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt-2-warm-up/standard-gpt/checkpoint-2007') # modify the path\n",
    "prompts = \"I was going to kill the dog at the\"\n",
    "inputs = tokenizer(prompts,return_tensors='pt').input_ids\n",
    "outputs = model.generate(inputs, max_new_tokens=100, do_sample=True, top_k=10, top_p=0.95)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8bcb23f8-c5ac-414c-9f8f-4b1109b8a669",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Granther/gpt-2-pretrained-26m/commit/db4a38bd2a789db4ba32fbecac815b788d1c7f19', commit_message='Upload tokenizer', commit_description='', oid='db4a38bd2a789db4ba32fbecac815b788d1c7f19', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Granther/gpt-2-pretrained-26m', endpoint='https://huggingface.co', repo_type='model', repo_id='Granther/gpt-2-pretrained-26m'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"Granther/gpt-2-pretrained-26m\")\n",
    "tokenizer.push_to_hub(\"Granther/gpt-2-pretrained-26m\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
