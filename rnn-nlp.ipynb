{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f970d9bf-180c-4e30-932a-dbdd22eef81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ead81f8f-b0e2-44d2-aa0d-bc66105f1329",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyper Params\n",
    "batch_size = 1\n",
    "hidden_size = 120\n",
    "input_size = 1\n",
    "lr = 0.001\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0c6db8d-1973-46bf-9e46-5b790efd56b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a3690d065924394954aaf8bf9be2fe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/404 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd4428eb68b143c98fe63f01375792ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(â€¦)-00000-of-00001-532ad934f217d092.parquet:   0%|          | 0.00/130k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83e74ab42a2247a69491e1c910e63c38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_dataset = load_dataset(\"jaydenccc/AI_Storyteller_Dataset\", split=\"train\")['short_story']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b889e196-d6a1-4fca-9c89-6e3c753e337d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/grant/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "\n",
    "text_corpus = \"\"\n",
    "for item in raw_dataset:\n",
    "    text_corpus += item.lower()\n",
    "\n",
    "tokens = word_tokenize(text_corpus)\n",
    "unique_words = set(tokens)\n",
    "word_to_num = {word: idx for idx, word in enumerate(unique_words)}\n",
    "num_to_word = {idx: word for idx, word in enumerate(unique_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b6fd7f9-58a7-47a5-9c6d-bbeee9b36698",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = len(word_to_num)\n",
    "\n",
    "pad_tok_id = output_size\n",
    "num_to_word[pad_tok_id] = '<pad>'\n",
    "word_to_num['<pad>'] = pad_tok_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "636a65fe-5b54-4f62-81a1-93a8744b7c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoryDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cabc52ba-4148-4020-a043-a7190e130852",
   "metadata": {},
   "outputs": [],
   "source": [
    "pads = ['<pad>' for i in range(9)]\n",
    "corpus = text_corpus.split(' ')\n",
    "\n",
    "train_dataset = StoryDataset(corpus[:25000])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "test_dataset = StoryDataset(corpus[25000:])\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d8185be7-c7e6-4bef-b0d8-19bbb1e3db86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['detective', 'emily', 'had', 'always', 'dreamt', 'of', 'getting', 'an', 'exciting', 'case']\n",
      "['to', 'solve.', 'her', 'wish', 'came', 'true', 'when', 'she', 'received', 'a']\n",
      "['call', 'about', 'a', 'series', 'of', 'bizarre', 'murders', 'in', 'a', 'small']\n",
      "['town', 'of', 'willow', 'creek.', 'emily', 'had', 'heard', 'about', 'this', 'town,']\n",
      "['known', 'for', 'its', 'friendly', 'community', 'and', 'green', 'landscape.', 'she', 'had']\n",
      "['never', 'imagined', 'such', 'horror', 'could', 'lurk', 'in', 'these', 'picturesque', 'streets.\\n\\nemily']\n",
      "['arrived', 'at', 'the', 'crime', 'scene,', 'a', 'small', 'cottage', 'on', 'the']\n",
      "['outskirts', 'of', 'town.', 'from', 'the', 'outside,', 'it', 'looked', 'like', 'any']\n",
      "['other', 'quaint', 'home,', 'but', 'inside,', 'it', 'was', 'a', 'bloodbath.', 'three']\n",
      "['people', 'had', 'been', 'brutally', 'murdered,', 'and', 'the', 'sight', 'was', 'enough']\n",
      "['to', 'make', 'her', 'stomach', 'turn.', 'there', 'were', 'no', 'signs', 'of']\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_dataloader):\n",
    "    # Give model i word, output should be id of next word. So loss will be item+1\n",
    "    if i > 10:\n",
    "        break\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e595e7d8-5cf4-4c6c-9b93-3be26c249e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Init hidden state (batch size * hidden size)\n",
    "        batch_size = x.size(0)\n",
    "        hidden = torch.zeros(1, batch_size, self.hidden_size) # \n",
    "\n",
    "        # Forward through net\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "\n",
    "        # Only use last hidden state for output\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "856c0b85-a19c-4b22-ba78-6abab9d9ac04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 240),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(240, output_size)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return torch.argmax(x, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "37fa4c9a-8494-4364-b780-9f815ee6f1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "crit = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4091ef80-642f-4f13-8a12-59d81f0c339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = net(torch.tensor([word_to_num['upon']], dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d08c2ca9-ee85-42b5-8c52-c792937ef1fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'solve.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataset):\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;66;03m# opt.zero_grad()\u001b[39;00m\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;66;03m# output = net(torch.tensor([word_to_num[item]], dtype=torch.float32))\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m         x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[43mword_to_num\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;66;03m# loss = crit(output, torch.tensor([word_to_num[train_dataset[i+1]]], dtype=torch.float32))\u001b[39;00m\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;66;03m# loss.backward()\u001b[39;00m\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;66;03m# opt.step()\u001b[39;00m\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;66;03m# print(loss.item())\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'solve.'"
     ]
    }
   ],
   "source": [
    "net.train()\n",
    "for epoch in range(epochs):\n",
    "    for i, item in enumerate(train_dataset):\n",
    "        # opt.zero_grad()\n",
    "        # output = net(torch.tensor([word_to_num[item]], dtype=torch.float32))\n",
    "        x = torch.tensor([word_to_num[train_dataset[i+1]]], dtype=torch.float32)\n",
    "        # loss = crit(output, torch.tensor([word_to_num[train_dataset[i+1]]], dtype=torch.float32))\n",
    "        # loss.backward()\n",
    "        # opt.step()\n",
    "        # print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "67e5d2be-2716-4474-ac06-07c0c068a9cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1124"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_num['solve']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3c90a9-c246-4846-8e76-6cb2f7a45ffa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
