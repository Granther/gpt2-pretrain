{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f970d9bf-180c-4e30-932a-dbdd22eef81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ead81f8f-b0e2-44d2-aa0d-bc66105f1329",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyper Params\n",
    "batch_size = 1\n",
    "hidden_size = 120\n",
    "input_size = 1\n",
    "lr = 0.01\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0c6db8d-1973-46bf-9e46-5b790efd56b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = load_dataset(\"jaydenccc/AI_Storyteller_Dataset\", split=\"train\")['short_story']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b889e196-d6a1-4fca-9c89-6e3c753e337d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/grant/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "\n",
    "text_corpus = \"\"\n",
    "for item in raw_dataset:\n",
    "    text_corpus += item.lower()\n",
    "\n",
    "text_corpus = text_corpus.replace('.', '').replace(',','').replace('\\n',' ').replace('\\'', '').replace('\\\"', '').replace(':', '').replace('?', '').replace('!', '').replace(';', '').replace('-', '')\n",
    "tokens = word_tokenize(text_corpus)\n",
    "unique_words = set(tokens)\n",
    "word_to_num = {word: idx for idx, word in enumerate(unique_words)}\n",
    "num_to_word = {idx: word for idx, word in enumerate(unique_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b6fd7f9-58a7-47a5-9c6d-bbeee9b36698",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = len(word_to_num)\n",
    "\n",
    "pad_tok_id = output_size\n",
    "num_to_word[pad_tok_id] = '<pad>'\n",
    "word_to_num['<pad>'] = pad_tok_id\n",
    "\n",
    "num_to_word[output_size+1] = ''\n",
    "word_to_num[''] = output_size+1\n",
    "\n",
    "output_size = len(word_to_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "636a65fe-5b54-4f62-81a1-93a8744b7c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoryDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cabc52ba-4148-4020-a043-a7190e130852",
   "metadata": {},
   "outputs": [],
   "source": [
    "pads = ['<pad>' for i in range(9)]\n",
    "corpus = text_corpus.split(' ')\n",
    "\n",
    "train_dataset = StoryDataset(corpus[:25000])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "test_dataset = StoryDataset(corpus[25000:])\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8185be7-c7e6-4bef-b0d8-19bbb1e3db86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['detective']\n",
      "['emily']\n",
      "['had']\n",
      "['always']\n",
      "['dreamt']\n",
      "['of']\n",
      "['getting']\n",
      "['an']\n",
      "['exciting']\n",
      "['case']\n",
      "['to']\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_dataloader):\n",
    "    # Give model i word, output should be id of next word. So loss will be item+1\n",
    "    if i > 10:\n",
    "        break\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e595e7d8-5cf4-4c6c-9b93-3be26c249e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Init hidden state (batch size * hidden size)\n",
    "        batch_size = x.size(0)\n",
    "        hidden = torch.zeros(1, batch_size, self.hidden_size) # \n",
    "\n",
    "        # Forward through net\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "\n",
    "        # Only use last hidden state for output\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "856c0b85-a19c-4b22-ba78-6abab9d9ac04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, 240),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(240, 480),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(480, output_size)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "        # return torch.argmax(x, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37fa4c9a-8494-4364-b780-9f815ee6f1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "crit = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4091ef80-642f-4f13-8a12-59d81f0c339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = net(torch.tensor([word_to_num['upon']], dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d08c2ca9-ee85-42b5-8c52-c792937ef1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 9.653342895507812, Step: 0/25000\n",
      "Loss: 555.5425284004211, Step: 100/25000\n",
      "Loss: 7.906805210113525, Step: 200/25000\n",
      "Loss: 7.366302399635315, Step: 300/25000\n",
      "Loss: 7.569762182235718, Step: 400/25000\n",
      "Loss: 7.373135266304016, Step: 500/25000\n",
      "Loss: 7.351459641456604, Step: 600/25000\n",
      "Loss: 7.286879057884216, Step: 700/25000\n",
      "Loss: 7.372102749347687, Step: 800/25000\n",
      "Loss: 7.443263423442841, Step: 900/25000\n",
      "Loss: 7.0087655687332155, Step: 1000/25000\n",
      "Loss: 7.52756477355957, Step: 1100/25000\n",
      "Loss: 13.511583476066589, Step: 1200/25000\n",
      "Loss: 7.370755836963654, Step: 1300/25000\n",
      "Loss: 7.559735224246979, Step: 1400/25000\n",
      "Loss: 7.590172894001007, Step: 1500/25000\n",
      "Loss: 7.11024962425232, Step: 1600/25000\n",
      "Loss: 7.334508609771729, Step: 1700/25000\n",
      "Loss: 7.546393492221832, Step: 1800/25000\n",
      "Loss: 7.436902322769165, Step: 1900/25000\n",
      "Loss: 7.579100074768067, Step: 2000/25000\n",
      "Loss: 6.888727729320526, Step: 2100/25000\n",
      "Loss: 7.247100560665131, Step: 2200/25000\n",
      "Loss: 8.19599015712738, Step: 2300/25000\n",
      "Loss: 7.586144495010376, Step: 2400/25000\n",
      "Loss: 7.487674162387848, Step: 2500/25000\n",
      "Loss: 7.282958371639252, Step: 2600/25000\n",
      "Loss: 7.364001660346985, Step: 2700/25000\n",
      "Loss: 7.062033753395081, Step: 2800/25000\n",
      "Loss: 6.9625155377388, Step: 2900/25000\n",
      "Loss: 7.166008322238922, Step: 3000/25000\n",
      "Loss: 7.263422486782074, Step: 3100/25000\n",
      "Loss: 6.794558398723602, Step: 3200/25000\n",
      "Loss: 7.172788407802582, Step: 3300/25000\n",
      "Loss: 6.891530079841614, Step: 3400/25000\n",
      "Loss: 6.915624184608459, Step: 3500/25000\n",
      "Loss: 7.407433598041535, Step: 3600/25000\n",
      "Loss: 7.192886364459992, Step: 3700/25000\n",
      "Loss: 7.389752242565155, Step: 3800/25000\n",
      "Loss: 7.003229548931122, Step: 3900/25000\n",
      "Loss: 7.122839395999908, Step: 4000/25000\n",
      "Loss: 7.05701523065567, Step: 4100/25000\n",
      "Loss: 7.0533534002304075, Step: 4200/25000\n",
      "Loss: 7.127341260910034, Step: 4300/25000\n",
      "Loss: 6.931599378585815, Step: 4400/25000\n",
      "Loss: 7.640733823776245, Step: 4500/25000\n",
      "Loss: 8.093095681667329, Step: 4600/25000\n",
      "Loss: 7.754881699085235, Step: 4700/25000\n",
      "Loss: 7.39169100522995, Step: 4800/25000\n",
      "Loss: 7.273509831428528, Step: 4900/25000\n",
      "Loss: 6.812887215614319, Step: 5000/25000\n",
      "Loss: 6.728223607540131, Step: 5100/25000\n",
      "Loss: 7.270012359619141, Step: 5200/25000\n",
      "Loss: 7.80922149181366, Step: 5300/25000\n",
      "Loss: 7.13121563911438, Step: 5400/25000\n",
      "Loss: 6.642084000110626, Step: 5500/25000\n",
      "Loss: 7.791391994953155, Step: 5600/25000\n",
      "Loss: 7.5788337516784665, Step: 5700/25000\n",
      "Loss: 8.041176447868347, Step: 5800/25000\n",
      "Loss: 7.5643721556663515, Step: 5900/25000\n",
      "Loss: 7.611341962814331, Step: 6000/25000\n",
      "Loss: 7.433831253051758, Step: 6100/25000\n",
      "Loss: 7.377983467578888, Step: 6200/25000\n",
      "Loss: 7.828038055896759, Step: 6300/25000\n",
      "Loss: 7.322076711654663, Step: 6400/25000\n",
      "Loss: 6.974012033939362, Step: 6500/25000\n",
      "Loss: 7.368526031970978, Step: 6600/25000\n",
      "Loss: 7.326555962562561, Step: 6700/25000\n",
      "Loss: 7.66629723072052, Step: 6800/25000\n",
      "Loss: 7.312065310478211, Step: 6900/25000\n",
      "Loss: 7.498990676403046, Step: 7000/25000\n",
      "Loss: 7.882972981929779, Step: 7100/25000\n",
      "Loss: 6.9385264468193055, Step: 7200/25000\n",
      "Loss: 7.803863625526429, Step: 7300/25000\n",
      "Loss: 7.484723873138428, Step: 7400/25000\n",
      "Loss: 8.464765455722809, Step: 7500/25000\n",
      "Loss: 6.932585642337799, Step: 7600/25000\n",
      "Loss: 7.4774232411384585, Step: 7700/25000\n",
      "Loss: 7.609814131259919, Step: 7800/25000\n",
      "Loss: 6.778480973243713, Step: 7900/25000\n",
      "Loss: 54.889504148960114, Step: 8000/25000\n",
      "Loss: 104.81222384214401, Step: 8100/25000\n",
      "Loss: 7.409031977653504, Step: 8200/25000\n",
      "Loss: 7.3655865609645845, Step: 8300/25000\n",
      "Loss: 7.237460341453552, Step: 8400/25000\n",
      "Loss: 7.837396204471588, Step: 8500/25000\n",
      "Loss: 7.265487501621246, Step: 8600/25000\n",
      "Loss: 8.151492598056793, Step: 8700/25000\n",
      "Loss: 7.5169877076148985, Step: 8800/25000\n",
      "Loss: 7.585380654335022, Step: 8900/25000\n",
      "Loss: 7.808185062408447, Step: 9000/25000\n",
      "Loss: 8.438696303367614, Step: 9100/25000\n",
      "Loss: 7.136160938739777, Step: 9200/25000\n",
      "Loss: 8.165532960891724, Step: 9300/25000\n",
      "Loss: 7.413651466369629, Step: 9400/25000\n",
      "Loss: 7.301327865123749, Step: 9500/25000\n",
      "Loss: 8.044986510276795, Step: 9600/25000\n",
      "Loss: 7.284041404724121, Step: 9700/25000\n",
      "Loss: 6.099601049423217, Step: 9800/25000\n",
      "Loss: 7.222862310409546, Step: 9900/25000\n",
      "Loss: 7.398336379528046, Step: 10000/25000\n",
      "Loss: 7.140171344280243, Step: 10100/25000\n",
      "Loss: 7.328010125160217, Step: 10200/25000\n",
      "Loss: 7.658008172512054, Step: 10300/25000\n",
      "Loss: 7.486869106292724, Step: 10400/25000\n",
      "Loss: 6.095413663387299, Step: 10500/25000\n",
      "Loss: 8.781719243526458, Step: 10600/25000\n",
      "Loss: 8.476137027740478, Step: 10700/25000\n",
      "Loss: 9.941636426448822, Step: 10800/25000\n",
      "Loss: 7.944294773340225, Step: 10900/25000\n",
      "Loss: 6.970358734130859, Step: 11000/25000\n",
      "Loss: 7.084183273315429, Step: 11100/25000\n",
      "Loss: 7.3259605169296265, Step: 11200/25000\n",
      "Loss: 14.758869318962097, Step: 11300/25000\n",
      "Loss: 8.476492321491241, Step: 11400/25000\n",
      "Loss: 7.567866404056549, Step: 11500/25000\n",
      "Loss: 8.344596209526062, Step: 11600/25000\n",
      "Loss: 8.928331866264344, Step: 11700/25000\n",
      "Loss: 7.9443626213073735, Step: 11800/25000\n",
      "Loss: 8.117084512710571, Step: 11900/25000\n",
      "Loss: 7.81893096446991, Step: 12000/25000\n",
      "Loss: 7.045163798332214, Step: 12100/25000\n",
      "Loss: 7.931930692195892, Step: 12200/25000\n",
      "Loss: 7.822074508666992, Step: 12300/25000\n",
      "Loss: 7.110152020454406, Step: 12400/25000\n",
      "Loss: 6.851393527984619, Step: 12500/25000\n",
      "Loss: 7.465147004127503, Step: 12600/25000\n",
      "Loss: 8.05943972349167, Step: 12700/25000\n",
      "Loss: 7.998104498386383, Step: 12800/25000\n",
      "Loss: 8.017847514152527, Step: 12900/25000\n",
      "Loss: 8.232905297279357, Step: 13000/25000\n",
      "Loss: 8.603728611469268, Step: 13100/25000\n",
      "Loss: 8.726180031299592, Step: 13200/25000\n",
      "Loss: 8.339699348211289, Step: 13300/25000\n",
      "Loss: 7.645001536607742, Step: 13400/25000\n",
      "Loss: 8.242494449615478, Step: 13500/25000\n",
      "Loss: 7.479782209396363, Step: 13600/25000\n",
      "Loss: 8.05846049785614, Step: 13700/25000\n",
      "Loss: 6.593509068489075, Step: 13800/25000\n",
      "Loss: 7.4271204376220705, Step: 13900/25000\n",
      "Loss: 7.999556324481964, Step: 14000/25000\n",
      "Loss: 7.850944344997406, Step: 14100/25000\n",
      "Loss: 7.803799200057983, Step: 14200/25000\n",
      "Loss: 7.959269812107086, Step: 14300/25000\n",
      "Loss: 7.733424081802368, Step: 14400/25000\n",
      "Loss: 7.624010257720947, Step: 14500/25000\n",
      "Loss: 6.999032528400421, Step: 14600/25000\n",
      "Loss: 6.95746465921402, Step: 14700/25000\n",
      "Loss: 6.537171862125397, Step: 14800/25000\n",
      "Loss: 8.392403316497802, Step: 14900/25000\n",
      "Loss: 7.862727918624878, Step: 15000/25000\n",
      "Loss: 9.350681235790253, Step: 15100/25000\n",
      "Loss: 7.777083631753921, Step: 15200/25000\n",
      "Loss: 7.508320057392121, Step: 15300/25000\n",
      "Loss: 8.113339581489562, Step: 15400/25000\n",
      "Loss: 8.205903372764588, Step: 15500/25000\n",
      "Loss: 7.80714278459549, Step: 15600/25000\n",
      "Loss: 8.254264485836028, Step: 15700/25000\n",
      "Loss: 7.217115032672882, Step: 15800/25000\n",
      "Loss: 7.844728624820709, Step: 15900/25000\n",
      "Loss: 8.274939193725587, Step: 16000/25000\n",
      "Loss: 6.827396683692932, Step: 16100/25000\n",
      "Loss: 9.418931269645691, Step: 16200/25000\n",
      "Loss: 7.917791907787323, Step: 16300/25000\n",
      "Loss: 7.957519159317017, Step: 16400/25000\n",
      "Loss: 7.2695157289505, Step: 16500/25000\n",
      "Loss: 7.898145964145661, Step: 16600/25000\n",
      "Loss: 8.302127978801728, Step: 16700/25000\n",
      "Loss: 7.962175223827362, Step: 16800/25000\n",
      "Loss: 8.462846863269807, Step: 16900/25000\n",
      "Loss: 8.697461273670196, Step: 17000/25000\n",
      "Loss: 8.018961870670319, Step: 17100/25000\n",
      "Loss: 8.124018440246582, Step: 17200/25000\n",
      "Loss: 8.358680534362794, Step: 17300/25000\n",
      "Loss: 7.0150135397911075, Step: 17400/25000\n",
      "Loss: 7.1707843542099, Step: 17500/25000\n",
      "Loss: 7.890095317363739, Step: 17600/25000\n",
      "Loss: 8.622648229599, Step: 17700/25000\n",
      "Loss: 7.254031953811645, Step: 17800/25000\n",
      "Loss: 6.613537023067474, Step: 17900/25000\n",
      "Loss: 8.516270577907562, Step: 18000/25000\n",
      "Loss: 8.408779919147491, Step: 18100/25000\n",
      "Loss: 7.625628354549408, Step: 18200/25000\n",
      "Loss: 8.483681313991546, Step: 18300/25000\n",
      "Loss: 7.465104916095734, Step: 18400/25000\n",
      "Loss: 8.383298201560974, Step: 18500/25000\n",
      "Loss: 8.755725409984588, Step: 18600/25000\n",
      "Loss: 8.084107744693757, Step: 18700/25000\n",
      "Loss: 8.215060901641845, Step: 18800/25000\n",
      "Loss: 7.81475067615509, Step: 18900/25000\n",
      "Loss: 8.109527060985565, Step: 19000/25000\n",
      "Loss: 6.586098976135254, Step: 19100/25000\n",
      "Loss: 7.304113147258758, Step: 19200/25000\n",
      "Loss: 7.366049301624298, Step: 19300/25000\n",
      "Loss: 7.654865741729736, Step: 19400/25000\n",
      "Loss: 7.82421452999115, Step: 19500/25000\n",
      "Loss: 8.985137495994568, Step: 19600/25000\n",
      "Loss: 9.060869345664978, Step: 19700/25000\n",
      "Loss: 8.75610378742218, Step: 19800/25000\n",
      "Loss: 8.441213483810424, Step: 19900/25000\n",
      "Loss: 7.561259341239929, Step: 20000/25000\n",
      "Loss: 6.784060547351837, Step: 20100/25000\n",
      "Loss: 8.265566914081573, Step: 20200/25000\n",
      "Loss: 7.790450439453125, Step: 20300/25000\n",
      "Loss: 9.443883697986603, Step: 20400/25000\n",
      "Loss: 8.933964636325836, Step: 20500/25000\n",
      "Loss: 7.352203435897827, Step: 20600/25000\n",
      "Loss: 8.099454653263091, Step: 20700/25000\n",
      "Loss: 7.891128215789795, Step: 20800/25000\n",
      "Loss: 9.134911139011383, Step: 20900/25000\n",
      "Loss: 8.848035304546356, Step: 21000/25000\n",
      "Loss: 6.912651460170746, Step: 21100/25000\n",
      "Loss: 9.234103937149047, Step: 21200/25000\n",
      "Loss: 8.836311507225037, Step: 21300/25000\n",
      "Loss: 8.498417119979859, Step: 21400/25000\n",
      "Loss: 8.353100185394288, Step: 21500/25000\n",
      "Loss: 8.528470640182496, Step: 21600/25000\n",
      "Loss: 7.7598833560943605, Step: 21700/25000\n",
      "Loss: 8.188180797100067, Step: 21800/25000\n",
      "Loss: 7.612670905590058, Step: 21900/25000\n",
      "Loss: 8.436721127033234, Step: 22000/25000\n",
      "Loss: 7.871554853916169, Step: 22100/25000\n",
      "Loss: 7.7101695084571835, Step: 22200/25000\n",
      "Loss: 8.834542858600617, Step: 22300/25000\n",
      "Loss: 7.905537819862365, Step: 22400/25000\n",
      "Loss: 8.320124843120574, Step: 22500/25000\n",
      "Loss: 6.892536911964417, Step: 22600/25000\n",
      "Loss: 7.62094096660614, Step: 22700/25000\n",
      "Loss: 6.567943739891052, Step: 22800/25000\n",
      "Loss: 7.211344878673554, Step: 22900/25000\n",
      "Loss: 7.559623472690582, Step: 23000/25000\n",
      "Loss: 7.432344036102295, Step: 23100/25000\n",
      "Loss: 7.587677571773529, Step: 23200/25000\n",
      "Loss: 8.437664382457733, Step: 23300/25000\n",
      "Loss: 8.9166889834404, Step: 23400/25000\n",
      "Loss: 7.336650745868683, Step: 23500/25000\n",
      "Loss: 8.873424797058105, Step: 23600/25000\n",
      "Loss: 8.579151632785797, Step: 23700/25000\n",
      "Loss: 10.226095390319824, Step: 23800/25000\n",
      "Loss: 7.778570313453674, Step: 23900/25000\n",
      "Loss: 7.414044864177704, Step: 24000/25000\n",
      "Loss: 6.853328630924225, Step: 24100/25000\n",
      "Loss: 7.154092569351196, Step: 24200/25000\n",
      "Loss: 9.260511796474457, Step: 24300/25000\n",
      "Loss: 6.636837067604065, Step: 24400/25000\n",
      "Loss: 7.640135700702667, Step: 24500/25000\n",
      "Loss: 7.829754965305328, Step: 24600/25000\n",
      "Loss: 7.5026756548881535, Step: 24700/25000\n",
      "Loss: 7.533384466171265, Step: 24800/25000\n",
      "Loss: 7.938850665092469, Step: 24900/25000\n",
      "Loss: 9.384716928005219, Step: 0/25000\n",
      "Loss: 7.461431589126587, Step: 100/25000\n",
      "Loss: 7.3152318906784055, Step: 200/25000\n",
      "Loss: 7.494538495540619, Step: 300/25000\n",
      "Loss: 7.885995988845825, Step: 400/25000\n",
      "Loss: 7.687802515029907, Step: 500/25000\n",
      "Loss: 7.9233593964576725, Step: 600/25000\n",
      "Loss: 7.110105519294739, Step: 700/25000\n",
      "Loss: 7.732463150024414, Step: 800/25000\n",
      "Loss: 7.5048377895355225, Step: 900/25000\n",
      "Loss: 7.4847509789466855, Step: 1000/25000\n",
      "Loss: 8.014503931999206, Step: 1100/25000\n",
      "Loss: 8.938409476280212, Step: 1200/25000\n",
      "Loss: 7.812784893512726, Step: 1300/25000\n",
      "Loss: 8.078123240470886, Step: 1400/25000\n",
      "Loss: 8.153388268947602, Step: 1500/25000\n",
      "Loss: 7.169789776802063, Step: 1600/25000\n",
      "Loss: 8.362441468238831, Step: 1700/25000\n",
      "Loss: 8.021870784759521, Step: 1800/25000\n",
      "Loss: 7.327872416973114, Step: 1900/25000\n",
      "Loss: 6.551501350402832, Step: 2000/25000\n",
      "Loss: 6.672079589366913, Step: 2100/25000\n",
      "Loss: 7.008993995189667, Step: 2200/25000\n",
      "Loss: 8.156380095481872, Step: 2300/25000\n",
      "Loss: 8.135008149147033, Step: 2400/25000\n",
      "Loss: 7.620712149143219, Step: 2500/25000\n",
      "Loss: 6.950958285331726, Step: 2600/25000\n",
      "Loss: 7.711243228912354, Step: 2700/25000\n",
      "Loss: 7.386141946315766, Step: 2800/25000\n",
      "Loss: 6.803135273456573, Step: 2900/25000\n",
      "Loss: 7.775926649570465, Step: 3000/25000\n",
      "Loss: 7.3987463140487675, Step: 3100/25000\n",
      "Loss: 7.520248324871063, Step: 3200/25000\n",
      "Loss: 6.957541589736938, Step: 3300/25000\n",
      "Loss: 6.692900836467743, Step: 3400/25000\n",
      "Loss: 7.187303307056427, Step: 3500/25000\n",
      "Loss: 6.840069127082825, Step: 3600/25000\n",
      "Loss: 7.418391962051391, Step: 3700/25000\n",
      "Loss: 7.586417119503022, Step: 3800/25000\n",
      "Loss: 7.142276699542999, Step: 3900/25000\n",
      "Loss: 7.586803441047668, Step: 4000/25000\n",
      "Loss: 7.6615439844131465, Step: 4100/25000\n",
      "Loss: 7.802698628902435, Step: 4200/25000\n",
      "Loss: 7.723751530647278, Step: 4300/25000\n",
      "Loss: 7.230537145137787, Step: 4400/25000\n",
      "Loss: 8.978898012638092, Step: 4500/25000\n",
      "Loss: 8.81057297706604, Step: 4600/25000\n",
      "Loss: 8.808604309558868, Step: 4700/25000\n",
      "Loss: 7.288434135913849, Step: 4800/25000\n",
      "Loss: 7.511682801246643, Step: 4900/25000\n",
      "Loss: 6.5786614513397215, Step: 5000/25000\n",
      "Loss: 7.024735441207886, Step: 5100/25000\n",
      "Loss: 7.539619982242584, Step: 5200/25000\n",
      "Loss: 8.269985177516936, Step: 5300/25000\n",
      "Loss: 7.303203780651092, Step: 5400/25000\n",
      "Loss: 6.572399854660034, Step: 5500/25000\n",
      "Loss: 7.900458469390869, Step: 5600/25000\n",
      "Loss: 7.937984583377838, Step: 5700/25000\n",
      "Loss: 8.302048139572143, Step: 5800/25000\n",
      "Loss: 7.5831105160713195, Step: 5900/25000\n",
      "Loss: 7.445864119529724, Step: 6000/25000\n",
      "Loss: 7.473995289802551, Step: 6100/25000\n",
      "Loss: 7.089914782047272, Step: 6200/25000\n",
      "Loss: 7.939766311645508, Step: 6300/25000\n",
      "Loss: 7.267061531543732, Step: 6400/25000\n",
      "Loss: 7.024073314666748, Step: 6500/25000\n",
      "Loss: 7.6506746387481686, Step: 6600/25000\n",
      "Loss: 7.195485541820526, Step: 6700/25000\n",
      "Loss: 8.200657720565795, Step: 6800/25000\n",
      "Loss: 7.372221348285675, Step: 6900/25000\n",
      "Loss: 7.452592844963074, Step: 7000/25000\n",
      "Loss: 7.578129644393921, Step: 7100/25000\n",
      "Loss: 6.676082172393799, Step: 7200/25000\n",
      "Loss: 7.822047760486603, Step: 7300/25000\n",
      "Loss: 7.345169966220856, Step: 7400/25000\n",
      "Loss: 8.58357001066208, Step: 7500/25000\n",
      "Loss: 6.829913930892944, Step: 7600/25000\n",
      "Loss: 7.617620432376862, Step: 7700/25000\n",
      "Loss: 7.889367861747742, Step: 7800/25000\n",
      "Loss: 6.947034194469452, Step: 7900/25000\n",
      "Loss: 7.576978693008423, Step: 8000/25000\n",
      "Loss: 7.636456427574157, Step: 8100/25000\n",
      "Loss: 7.376265008449554, Step: 8200/25000\n",
      "Loss: 7.537102687358856, Step: 8300/25000\n",
      "Loss: 7.349419374465942, Step: 8400/25000\n",
      "Loss: 7.903366887569428, Step: 8500/25000\n",
      "Loss: 7.317780084609986, Step: 8600/25000\n",
      "Loss: 8.043263714313508, Step: 8700/25000\n",
      "Loss: 7.226670517921447, Step: 8800/25000\n",
      "Loss: 7.509057869911194, Step: 8900/25000\n",
      "Loss: 7.815031609535217, Step: 9000/25000\n",
      "Loss: 8.3782949924469, Step: 9100/25000\n",
      "Loss: 6.930377559661865, Step: 9200/25000\n",
      "Loss: 7.94675684928894, Step: 9300/25000\n",
      "Loss: 7.2600070667266845, Step: 9400/25000\n",
      "Loss: 6.959373579025269, Step: 9500/25000\n",
      "Loss: 7.972114634513855, Step: 9600/25000\n",
      "Loss: 7.155987074375153, Step: 9700/25000\n",
      "Loss: 5.985420489311219, Step: 9800/25000\n",
      "Loss: 7.442450528144836, Step: 9900/25000\n",
      "Loss: 7.125332181453705, Step: 10000/25000\n",
      "Loss: 7.1010614776611325, Step: 10100/25000\n",
      "Loss: 7.1300763630867, Step: 10200/25000\n",
      "Loss: 7.414306445121765, Step: 10300/25000\n",
      "Loss: 7.357311315536499, Step: 10400/25000\n",
      "Loss: 6.106035578250885, Step: 10500/25000\n",
      "Loss: 8.534273781776427, Step: 10600/25000\n",
      "Loss: 8.325196945667267, Step: 10700/25000\n",
      "Loss: 8.473287143707275, Step: 10800/25000\n",
      "Loss: 7.721162414550781, Step: 10900/25000\n",
      "Loss: 7.016266911029816, Step: 11000/25000\n",
      "Loss: 6.940730385780334, Step: 11100/25000\n",
      "Loss: 7.268452939987182, Step: 11200/25000\n",
      "Loss: 7.135752849578857, Step: 11300/25000\n",
      "Loss: 6.93929890871048, Step: 11400/25000\n",
      "Loss: 7.781282291412354, Step: 11500/25000\n",
      "Loss: 8.444939494132996, Step: 11600/25000\n",
      "Loss: 8.817789080142974, Step: 11700/25000\n",
      "Loss: 7.825757260322571, Step: 11800/25000\n",
      "Loss: 7.93686265707016, Step: 11900/25000\n",
      "Loss: 7.415756094455719, Step: 12000/25000\n",
      "Loss: 7.093463487625122, Step: 12100/25000\n",
      "Loss: 7.850796961784363, Step: 12200/25000\n",
      "Loss: 7.817780647277832, Step: 12300/25000\n",
      "Loss: 6.958081841468811, Step: 12400/25000\n",
      "Loss: 6.66546772480011, Step: 12500/25000\n",
      "Loss: 7.361176905632019, Step: 12600/25000\n",
      "Loss: 7.917440481185913, Step: 12700/25000\n",
      "Loss: 7.487319796085358, Step: 12800/25000\n",
      "Loss: 7.958642699718475, Step: 12900/25000\n",
      "Loss: 8.03731529712677, Step: 13000/25000\n",
      "Loss: 8.387795782089233, Step: 13100/25000\n",
      "Loss: 8.23508803844452, Step: 13200/25000\n",
      "Loss: 8.06628315448761, Step: 13300/25000\n",
      "Loss: 7.5283650255203245, Step: 13400/25000\n",
      "Loss: 7.916902265548706, Step: 13500/25000\n",
      "Loss: 7.130585215091705, Step: 13600/25000\n",
      "Loss: 7.721568174362183, Step: 13700/25000\n",
      "Loss: 6.363211877346039, Step: 13800/25000\n",
      "Loss: 7.2601212620735165, Step: 13900/25000\n",
      "Loss: 7.713481168746949, Step: 14000/25000\n",
      "Loss: 7.546489970684052, Step: 14100/25000\n",
      "Loss: 7.647885875701904, Step: 14200/25000\n",
      "Loss: 7.8005042958259585, Step: 14300/25000\n",
      "Loss: 7.591290028095245, Step: 14400/25000\n",
      "Loss: 7.381082804203033, Step: 14500/25000\n",
      "Loss: 6.840750515460968, Step: 14600/25000\n",
      "Loss: 6.799676465988159, Step: 14700/25000\n",
      "Loss: 6.391074523925782, Step: 14800/25000\n",
      "Loss: 7.7931409740448, Step: 14900/25000\n",
      "Loss: 7.344512116909027, Step: 15000/25000\n",
      "Loss: 8.751567986011505, Step: 15100/25000\n",
      "Loss: 7.259182107448578, Step: 15200/25000\n",
      "Loss: 7.126978650093078, Step: 15300/25000\n",
      "Loss: 7.74858960390091, Step: 15400/25000\n",
      "Loss: 7.908784909248352, Step: 15500/25000\n",
      "Loss: 7.423789899349213, Step: 15600/25000\n",
      "Loss: 7.812989869117737, Step: 15700/25000\n",
      "Loss: 6.821617028713226, Step: 15800/25000\n",
      "Loss: 7.494761567115784, Step: 15900/25000\n",
      "Loss: 7.915390331745147, Step: 16000/25000\n",
      "Loss: 6.620647642612457, Step: 16100/25000\n",
      "Loss: 8.86417682647705, Step: 16200/25000\n",
      "Loss: 7.5743335080146785, Step: 16300/25000\n",
      "Loss: 7.636877911090851, Step: 16400/25000\n",
      "Loss: 7.102231736183167, Step: 16500/25000\n",
      "Loss: 7.385267705917358, Step: 16600/25000\n",
      "Loss: 7.831284787654877, Step: 16700/25000\n",
      "Loss: 7.648532192707062, Step: 16800/25000\n",
      "Loss: 8.024325592517853, Step: 16900/25000\n",
      "Loss: 8.263995802402496, Step: 17000/25000\n",
      "Loss: 7.475621247291565, Step: 17100/25000\n",
      "Loss: 7.809355530738831, Step: 17200/25000\n",
      "Loss: 7.8464493083953855, Step: 17300/25000\n",
      "Loss: 6.798485662937164, Step: 17400/25000\n",
      "Loss: 6.919664027690888, Step: 17500/25000\n",
      "Loss: 7.566812326908112, Step: 17600/25000\n",
      "Loss: 8.212881121635437, Step: 17700/25000\n",
      "Loss: 6.993861787319183, Step: 17800/25000\n",
      "Loss: 6.526087458133698, Step: 17900/25000\n",
      "Loss: 8.115400969982147, Step: 18000/25000\n",
      "Loss: 7.945163295269013, Step: 18100/25000\n",
      "Loss: 7.283313164710998, Step: 18200/25000\n",
      "Loss: 7.859769425392151, Step: 18300/25000\n",
      "Loss: 7.023062977790833, Step: 18400/25000\n",
      "Loss: 7.823736124038696, Step: 18500/25000\n",
      "Loss: 8.10370611190796, Step: 18600/25000\n",
      "Loss: 7.478764307498932, Step: 18700/25000\n",
      "Loss: 7.817467949390411, Step: 18800/25000\n",
      "Loss: 7.482962098121643, Step: 18900/25000\n",
      "Loss: 7.78547370672226, Step: 19000/25000\n",
      "Loss: 6.271398456096649, Step: 19100/25000\n",
      "Loss: 6.9446412944793705, Step: 19200/25000\n",
      "Loss: 6.92579154253006, Step: 19300/25000\n",
      "Loss: 7.193176429271698, Step: 19400/25000\n",
      "Loss: 7.486187980175019, Step: 19500/25000\n",
      "Loss: 8.557526640892029, Step: 19600/25000\n",
      "Loss: 8.496178314685821, Step: 19700/25000\n",
      "Loss: 8.217147128582, Step: 19800/25000\n",
      "Loss: 7.903113090991974, Step: 19900/25000\n",
      "Loss: 7.243555860519409, Step: 20000/25000\n",
      "Loss: 6.537536854743958, Step: 20100/25000\n",
      "Loss: 7.720046284198761, Step: 20200/25000\n",
      "Loss: 7.272971265316009, Step: 20300/25000\n",
      "Loss: 8.676532573699951, Step: 20400/25000\n",
      "Loss: 8.461075954437256, Step: 20500/25000\n",
      "Loss: 7.01884512424469, Step: 20600/25000\n",
      "Loss: 7.696059980392456, Step: 20700/25000\n",
      "Loss: 7.364948711395264, Step: 20800/25000\n",
      "Loss: 8.494469525814056, Step: 20900/25000\n",
      "Loss: 8.247734656333924, Step: 21000/25000\n",
      "Loss: 6.631914350986481, Step: 21100/25000\n",
      "Loss: 8.606694824695587, Step: 21200/25000\n",
      "Loss: 8.14955893754959, Step: 21300/25000\n",
      "Loss: 7.934579117298126, Step: 21400/25000\n",
      "Loss: 7.797546172142029, Step: 21500/25000\n",
      "Loss: 7.906916987895966, Step: 21600/25000\n",
      "Loss: 7.268646240234375, Step: 21700/25000\n",
      "Loss: 7.675006287097931, Step: 21800/25000\n",
      "Loss: 7.163305213451386, Step: 21900/25000\n",
      "Loss: 7.785541684627533, Step: 22000/25000\n",
      "Loss: 7.2285477471351625, Step: 22100/25000\n",
      "Loss: 7.271849820613861, Step: 22200/25000\n",
      "Loss: 8.284671113491058, Step: 22300/25000\n",
      "Loss: 7.5922540044784546, Step: 22400/25000\n",
      "Loss: 7.770897450447083, Step: 22500/25000\n",
      "Loss: 6.704035222530365, Step: 22600/25000\n",
      "Loss: 7.131045835018158, Step: 22700/25000\n",
      "Loss: 6.43782386302948, Step: 22800/25000\n",
      "Loss: 6.944237213134766, Step: 22900/25000\n",
      "Loss: 7.135413854122162, Step: 23000/25000\n",
      "Loss: 7.035097703933716, Step: 23100/25000\n",
      "Loss: 7.191503002643585, Step: 23200/25000\n",
      "Loss: 7.826908364295959, Step: 23300/25000\n",
      "Loss: 8.100684444904328, Step: 23400/25000\n",
      "Loss: 6.692583577632904, Step: 23500/25000\n",
      "Loss: 8.09700044631958, Step: 23600/25000\n",
      "Loss: 7.945534605979919, Step: 23700/25000\n",
      "Loss: 9.280777127742768, Step: 23800/25000\n",
      "Loss: 7.3103994011878966, Step: 23900/25000\n",
      "Loss: 6.994292459487915, Step: 24000/25000\n",
      "Loss: 6.460824286937713, Step: 24100/25000\n",
      "Loss: 6.888587651252746, Step: 24200/25000\n",
      "Loss: 8.51772224187851, Step: 24300/25000\n",
      "Loss: 6.279442138671875, Step: 24400/25000\n",
      "Loss: 7.116493225097656, Step: 24500/25000\n",
      "Loss: 7.342041358947754, Step: 24600/25000\n",
      "Loss: 7.067681839466095, Step: 24700/25000\n",
      "Loss: 7.145257627964019, Step: 24800/25000\n",
      "Loss: 7.481548442840576, Step: 24900/25000\n"
     ]
    }
   ],
   "source": [
    "# net.train()\n",
    "# for epoch in range(epochs):\n",
    "#     for i, item in enumerate(train_dataset):\n",
    "#         if i < len(train_dataset)-1:\n",
    "#             opt.zero_grad()\n",
    "#             output = net(torch.tensor([word_to_num[item]], dtype=torch.float32))\n",
    "#             label = torch.tensor(word_to_num[train_dataset[i+1]], dtype=torch.float32)\n",
    "\n",
    "#             print(output, label)\n",
    "            \n",
    "#             loss = crit(output, label)\n",
    "#             loss.backward()\n",
    "#             opt.step()\n",
    "#             print(loss.item())\n",
    "avg_loss = 0\n",
    "net.train()\n",
    "for epoch in range(epochs):\n",
    "    for i, item in enumerate(train_dataset):\n",
    "        if i < len(train_dataset) - 1:\n",
    "            opt.zero_grad()\n",
    "            \n",
    "            # Prepare input and target\n",
    "            input_tensor = torch.tensor([[word_to_num[item]]], dtype=torch.float32)\n",
    "            target_tensor = torch.tensor([word_to_num[train_dataset[i+1]]], dtype=torch.long)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = net(input_tensor)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = crit(output, target_tensor)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            avg_loss += loss.item()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f'Loss: {avg_loss/100}, Step: {i}/{len(train_dataset)}')\n",
    "            avg_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e3ced6b-9c1d-410d-939c-fbe6e458dbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './next_word_2.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd3c90a9-c246-4846-8e76-6cb2f7a45ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: the\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "input_tensor = torch.tensor([[word_to_num[\"the\"]]], dtype=torch.float32)\n",
    "\n",
    "# Forward pass\n",
    "output = torch.argmax(net(input_tensor))\n",
    "print(f'Word: {num_to_word[output.item()]}')\n",
    "\n",
    "plot = output.detach().numpy()\n",
    "plt.plot(plot)\n",
    "plt.show()\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.bar(range(len(fc1_logits[0])), fc1_logits[0], color='blue', alpha=0.7)\n",
    "# plt.title(\"Logits from fc1 Layer\")\n",
    "# plt.xlabel(\"Neuron Index\")\n",
    "# plt.ylabel(\"Logit Value\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fbec7a-1e28-4a0b-81cf-db2ed8545fef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
