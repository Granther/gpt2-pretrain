{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff2aba62-7f49-4dfb-be6c-28c966aed4d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'accelerator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01maccelerator\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# torch\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'accelerator'"
     ]
    }
   ],
   "source": [
    "from itertools import islice, chain\n",
    "import warnings\n",
    "import math\n",
    "\n",
    "#from hf\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, GPT2Model\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import get_scheduler\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "import accelerator\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77be6c4b-fceb-4b4e-8a03-cd51cce48588",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyper Params\n",
    "learning_rate = 2.5e-4\n",
    "warmup_ratio = 0.5\n",
    "num_rows = 1000\n",
    "context_len = 1024\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffbae897-d02a-47da-8858-a04c75d35eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "564354b0022443899d6cda300893567b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/23781 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e2c4f52936c47e38f763239a11444aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/23781 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loading raw data\n",
    "dataset_stream = load_dataset(\"HuggingFaceFW/fineweb\", split=\"train\", streaming=True)\n",
    "\n",
    "dataset_raw = Dataset.from_list(list(islice(dataset_stream, num_rows)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cdec6f-85d3-439b-8c68-e3cf18152c64",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token=tokenizer.eos_token\n",
    "\n",
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element['text'],\n",
    "        truncation=True,\n",
    "        max_length=context_len,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs['length'], outputs['input_ids']):\n",
    "        if length == context_len:\n",
    "            input_batch.append(input_ids)\n",
    "\n",
    "    # print(len(input_batch))\n",
    "    return {\"input_ids\": torch.tensor(input_batch)}\n",
    "\n",
    "tokenized_ds = dataset_raw.map(tokenize, remove_columns=dataset_raw.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d97fb71-c9c6-4b74-a53a-f4774c3a188e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the gpt-2 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token=tokenizer.eos_token\n",
    "\n",
    "# tokenize\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(text=example[\"text\"])\n",
    "    \n",
    "# tokenized_ds = dataset.map(tokenize_function, batched=True, remove_columns='text')\n",
    "#tokenized_ds = dataset_raw.map(tokenize_function, batched=True, remove_columns=['text', 'id', 'dump', 'url', 'date', 'file_path', 'language', 'language_score', 'token_count'], num_proc=8)\n",
    "\n",
    "# save to disk if required (use load_from_disk latter)\n",
    "# tokenized_ds.save_to_disk('bookcorpus/tokenized_ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dced70bf-568b-456d-8ba6-edf4907f7bd5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "951dbaa096b84d67929eda1e739eb548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def chunk_and_pad(examples):\n",
    "    chunk_size = 1024  # Replace with model's max input length\n",
    "    input_ids = examples[\"input_ids\"]\n",
    "\n",
    "    # Create chunks\n",
    "    chunks = [input_ids[i:i + chunk_size] for i in range(0, len(input_ids), chunk_size)]\n",
    "\n",
    "    # Drop the last chunk if it's smaller than `chunk_size`\n",
    "    chunks = [chunk for chunk in chunks if len(chunk) == chunk_size]\n",
    "\n",
    "    return {\"input_ids\": chunks}\n",
    "\n",
    "# Apply chunking\n",
    "chunked_ds = tokenized_ds.map(chunk_and_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3a8c1ff-db06-4291-a081-315799b4a6b5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6f03b1d8e884f5f8a216ee90aacd945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def chunk(examples):\n",
    "    chunk_size = 1024 # modify this accordingly       \n",
    "    input_ids = examples[\"input_ids\"] # List[List], pass the inner list      \n",
    "    attention_mask = examples[\"attention_mask\"] # List[List]\n",
    "    input_ids_truncated = []\n",
    "    attention_mask_truncated = []\n",
    "    \n",
    "    #slice with step_size=chunk_size\n",
    "    for i in range(0,len(input_ids),chunk_size):\n",
    "        chunk = input_ids[i:i+chunk_size]\n",
    "        if len(chunk)==chunk_size: # drop the last chunk if not equal\n",
    "            input_ids_truncated.append(chunk)\n",
    "            attention_mask_truncated.append(attention_mask[i:i+chunk_size])     \n",
    "    examples['input_ids']=input_ids_truncated\n",
    "    examples[\"attention_mask\"]=attention_mask_truncated\n",
    "    \n",
    "    return examples   \n",
    "\n",
    "chunked_ds = tokenized_ds.map(chunk)\n",
    "#,batched=True,batch_size=2,num_proc=2)\n",
    "# chunked_ds.save_to_disk('bookcorpus/chunked_ds') # will use this latter for diff experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ceb2ee7e-54c5-4029-87bf-1140214656b7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "def to_torch_format(examples):\n",
    "    return {\"input_ids\": torch.tensor(examples[\"input_ids\"], dtype=torch.long)}\n",
    "\n",
    "tensor_ds = tokenized_ds.with_transform(to_torch_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "413893eb-a488-4e78-9be6-aa57bd74dcc1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(tensor_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ae62c1-1aa4-48c4-a887-0eb58b431983",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ds['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "411cfcff-8669-453d-903f-ab5227c64455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d21dee48ef6f4ae6b54300b30a03443a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save to disk if required (use load_from_disk latter)\n",
    "# tokenized_ds.save_to_disk('bookcorpus/tokenized_ds')\n",
    "\n",
    "# Make samples to a size of 1024\n",
    "# def concat(examples):    \n",
    "#     examples[\"input_ids\"]=[list(chain.from_iterable(examples['input_ids']))] # convert chain to list of tokens\n",
    "#     examples[\"attention_mask\"]=[list(chain.from_iterable(examples['attention_mask']))] # convert chain to list of tokens\n",
    "#     return examples\n",
    "    \n",
    "# # takes a lot of time (worth saving it to disk)\n",
    "# concated_ds = tokenized_ds.map(concat,num_proc=8)\n",
    "\n",
    "def chunk(examples):\n",
    "    chunk_size = 1024 # modify this accordingly       \n",
    "    input_ids = examples[\"input_ids\"][0] # List[List], pass the inner list      \n",
    "    attention_mask = examples[\"attention_mask\"][0] # List[List]\n",
    "    input_ids_truncated = []\n",
    "    attention_mask_truncated = []\n",
    "    \n",
    "    #slice with step_size=chunk_size\n",
    "    for i in range(0,len(input_ids),chunk_size):\n",
    "        chunk = input_ids[i:i+chunk_size]\n",
    "        if len(chunk)==chunk_size: # drop the last chunk if not equal\n",
    "            input_ids_truncated.append(chunk)\n",
    "            attention_mask_truncated.append(attention_mask[i:i+chunk_size])     \n",
    "    examples['input_ids']=input_ids_truncated\n",
    "    examples[\"attention_mask\"]=attention_mask_truncated\n",
    "        \n",
    "    return examples   \n",
    "\n",
    "chunked_ds = tokenized_ds.map(chunk,batched=True,batch_size=2,num_proc=2)\n",
    "# chunked_ds.save_to_disk('bookcorpus/chunked_ds') # will use this lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04403d8d-ac41-4380-b954-82e080c1f727",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer,mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3959975-95d2-4b38-8112-198a421758dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09d6007e-99c4-49e7-bdfb-6d27a1a38717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    chunked_ds, \n",
    "    batch_size=2,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b5046f0-800a-4763-8ecd-0071e23d3a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42,564,150\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "configuration = GPT2Config(\n",
    "    n_head=6,\n",
    "    n_embd=450,\n",
    "    n_layer=8,\n",
    ")\n",
    "model = GPT2LMHeadModel(configuration)\n",
    "model.to(device)\n",
    "print(f\"{model.num_parameters():,}\")\n",
    "\n",
    "# # training arguments\n",
    "# training_args = TrainingArguments( output_dir='gpt-2-warm-up/standard-gpt',\n",
    "#                                   # evaluation_strategy=\"steps\",\n",
    "#                                   # eval_steps=500,                                  \n",
    "#                                   num_train_epochs=1,\n",
    "#                                   per_device_train_batch_size=4,\n",
    "#                                   # per_device_eval_batch_size=8,\n",
    "#                                   learning_rate=2.5e-4,\n",
    "#                                   lr_scheduler_type='cosine',\n",
    "#                                   warmup_ratio=0.05,\n",
    "#                                   adam_beta1=0.9,\n",
    "#                                   adam_beta2=0.999,                                  \n",
    "#                                   weight_decay=0.01,                                  \n",
    "#                                   logging_strategy=\"steps\",\n",
    "#                                   logging_steps = 50,\n",
    "#                                   save_steps=10,\n",
    "#                                   save_total_limit=10,                                  \n",
    "#                                  ) \n",
    "# trainer = Trainer(model=model,\n",
    "#                  args = training_args,\n",
    "#                  tokenizer=tokenizer,\n",
    "#                  train_dataset=chunked_ds,\n",
    "#                  # eval_dataset=chunked_ds,\n",
    "#                  data_collator = data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4415c3f-9439-4855-ab77-2dcc3c8491a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "scheduler = get_scheduler(\n",
    "    name=\"cosine\",\n",
    "    optimizer=optimizer, \n",
    "    num_warmup_steps=5,\n",
    "    num_training_steps=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ff1233-2a2b-4dd5-ade8-c77955c5a39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "for i in range(epochs):\n",
    "    for step, batch in enumerate(data_loader):\n",
    "        batch = {key: value.to(device) for key, value in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward(loss)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "#     outputs = model(**batch)\n",
    "#     loss = outputs.loss\n",
    "#     # acc.backward(loss)\n",
    "#     # optimizer.step()\n",
    "#     # scheduler.step()\n",
    "#     # optimizer.zero_grad()\n",
    "\n",
    "#     if i > 10:\n",
    "#         break\n",
    "    \n",
    "    # if acc.is_main_process:\n",
    "    # perplexity = torch.exp(loss)\n",
    "    # wandb.log({\"loss\": loss.item(), \"learning_rate\": optimizer.param_groups[0]['lr'], \"perplexity\": perplexity})\n",
    "    \n",
    "    # global_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30364ffd-3b81-4057-bc59-d65a5916d622",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Cock and ball tortureracks , and a small round table in the center .a couple of chairs sat down on the opposite side of the table .`` i 've never seen anyone before , '' the guy said .`` i 've seen the place . ''the bartender said , `` i think they 're here . ''`` i 'll take that to them . ''he turned and started to move toward the door , `` but if you 'll be able to get them out of the house , they '\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('Granther/gpt2-pretrain-bookcorp-40m') # modify the path\n",
    "prompts = \"Cock and ball torture\"\n",
    "inputs = tokenizer(prompts,return_tensors='pt').input_ids\n",
    "outputs = model.generate(inputs, max_new_tokens=100, do_sample=True, top_k=10, top_p=0.85)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcb23f8-c5ac-414c-9f8f-4b1109b8a669",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\"Granther/gpt-2-pretrained-26m\")\n",
    "tokenizer.push_to_hub(\"Granther/gpt-2-pretrained-26m\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
