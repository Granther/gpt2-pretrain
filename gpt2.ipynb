{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff2aba62-7f49-4dfb-be6c-28c966aed4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import warnings\n",
    "import math\n",
    "\n",
    "#from hf\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, GPT2Model\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a89c0a9f-66bc-42c0-9c28-8a23aaa43f3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9485b2f36b4daf9c7acd6486ab1283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "865a70a725174edfb40542f055e59e28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/200000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "871140b8d9974875b8f162bc3ee4fd64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/200000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28a8aa487ac44d7d97354f2c038a6031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef598e08f354d25aed39a3101818b28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1491 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 1491\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading raw data\n",
    "dataset = load_dataset(\"raddwolf/BookCorpus74M\",trust_remote_code=True)\n",
    "\n",
    "# make splits\n",
    "dataset = dataset['train'].select(range(200000))\n",
    "\n",
    "dataset.train_test_split(test_size=0.0015) \n",
    "\n",
    "# load the gpt-2 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token=tokenizer.eos_token\n",
    "\n",
    "# tokenize\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(text=example[\"text\"])\n",
    "tokenized_ds = dataset.map(tokenize_function,batched=True,remove_columns='text')\n",
    "\n",
    "# save to disk if required (use load_from_disk latter)\n",
    "tokenized_ds.save_to_disk('bookcorpus/tokenized_ds')\n",
    "\n",
    "# Make samples to a size of 1024\n",
    "def concat(examples):    \n",
    "    examples[\"input_ids\"]=[list(chain.from_iterable(examples['input_ids']))] # convert chain to list of tokens\n",
    "    examples[\"attention_mask\"]=[list(chain.from_iterable(examples['attention_mask']))] # convert chain to list of tokens\n",
    "    return examples\n",
    "    \n",
    "# takes a lot of time (worth saving it to disk)\n",
    "concated_ds = tokenized_ds.map(concat,batched=True,batch_size=1000000,num_proc=8)\n",
    "\n",
    "def chunk(examples):\n",
    "    chunk_size = 1024 # modify this accordingly       \n",
    "    input_ids = examples[\"input_ids\"][0] # List[List], pass the inner list      \n",
    "    attention_mask = examples[\"attention_mask\"][0] # List[List]\n",
    "    input_ids_truncated = []\n",
    "    attention_mask_truncated = []\n",
    "    \n",
    "    #slice with step_size=chunk_size\n",
    "    for i in range(0,len(input_ids),chunk_size):\n",
    "        chunk = input_ids[i:i+chunk_size]\n",
    "        if len(chunk)==chunk_size: # drop the last chunk if not equal\n",
    "            input_ids_truncated.append(chunk)\n",
    "            attention_mask_truncated.append(attention_mask[i:i+chunk_size])     \n",
    "    examples['input_ids']=input_ids_truncated\n",
    "    examples[\"attention_mask\"]=attention_mask_truncated\n",
    "        \n",
    "    return examples   \n",
    "\n",
    "chunked_ds = concated_ds.map(chunk,batched=True,batch_size=2,num_proc=2)\n",
    "chunked_ds.save_to_disk('bookcorpus/chunked_ds') # will use this latter for diff experimentation\n",
    "\n",
    "chunked_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04403d8d-ac41-4380-b954-82e080c1f727",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer,mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b5046f0-800a-4763-8ecd-0071e23d3a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26,790,528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82342/2452780454.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(model=model,\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "configuration = GPT2Config(\n",
    "    n_head=6,\n",
    "    n_embd=384,\n",
    "    n_layer=4,\n",
    ")\n",
    "model = GPT2LMHeadModel(configuration)\n",
    "# model = GPT2Model(configuration)\n",
    "print(f\"{model.num_parameters():,}\")\n",
    "\n",
    "# training arguments\n",
    "training_args = TrainingArguments( output_dir='gpt-2-warm-up/standard-gpt',\n",
    "                                  # evaluation_strategy=\"steps\",\n",
    "                                  # eval_steps=500,                                  \n",
    "                                  num_train_epochs=1,\n",
    "                                  per_device_train_batch_size=8,\n",
    "                                  # per_device_eval_batch_size=8,\n",
    "                                  learning_rate=2.5e-4,\n",
    "                                  lr_scheduler_type='cosine',\n",
    "                                  warmup_ratio=0.05,\n",
    "                                  adam_beta1=0.9,\n",
    "                                  adam_beta2=0.999,                                  \n",
    "                                  weight_decay=0.01,                                  \n",
    "                                  logging_strategy=\"steps\",\n",
    "                                  logging_steps = 5,\n",
    "                                  save_steps=20,\n",
    "                                  save_total_limit=10,                                  \n",
    "                                 ) \n",
    "trainer = Trainer(model=model,\n",
    "                 args = training_args,\n",
    "                 tokenizer=tokenizer,\n",
    "                 train_dataset=chunked_ds,\n",
    "                 # eval_dataset=chunked_ds,\n",
    "                 data_collator = data_collator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2f41ec51-623d-437a-8e72-92bcb5eddeb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='187' max='187' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [187/187 34:35, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>10.694500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>9.974800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>9.430100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>8.737300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>8.108600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>7.502800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>7.046200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>6.716200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>6.399800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>6.190500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>6.100900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>5.991500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>5.858400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>5.851100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>5.745500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>5.697000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>5.630100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>5.592900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>5.550600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.585800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>5.547900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>5.474800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>5.450800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>5.515600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>5.501900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>5.354600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>5.461300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>5.384000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>5.400900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>5.415300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>5.387700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>5.301900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>5.436800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>5.311500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>5.330900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>5.316600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>5.396100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=187, training_loss=6.217721276104769, metrics={'train_runtime': 2087.6931, 'train_samples_per_second': 0.714, 'train_steps_per_second': 0.09, 'total_flos': 65028393271296.0, 'train_loss': 6.217721276104769, 'epoch': 1.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "30364ffd-3b81-4057-bc59-d65a5916d622",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"I was telling her that she was he her her her , .she . ''`` her , but she , and the the he 'd to her her and the the she 'd she did n't and her and she 's she 's .she was her . ''he .`` i 's , .`` i 'd .i was the , she was and her .he was the that she was she was n't was n't the his .she , he was .`` you .she was to\"]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt-2-warm-up/standard-gpt/checkpoint-98') # modify the path\n",
    "prompts = \"I was telling her that\"\n",
    "inputs = tokenizer(prompts,return_tensors='pt').input_ids\n",
    "outputs = model.generate(inputs, max_new_tokens=100, do_sample=True, top_k=10, top_p=0.95)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8bcb23f8-c5ac-414c-9f8f-4b1109b8a669",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Granther/gpt-2-pretrained-26m/commit/db4a38bd2a789db4ba32fbecac815b788d1c7f19', commit_message='Upload tokenizer', commit_description='', oid='db4a38bd2a789db4ba32fbecac815b788d1c7f19', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Granther/gpt-2-pretrained-26m', endpoint='https://huggingface.co', repo_type='model', repo_id='Granther/gpt-2-pretrained-26m'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"Granther/gpt-2-pretrained-26m\")\n",
    "tokenizer.push_to_hub(\"Granther/gpt-2-pretrained-26m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366ed298-c35f-4519-9c90-7a24fb449ec1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
