{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f970d9bf-180c-4e30-932a-dbdd22eef81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ead81f8f-b0e2-44d2-aa0d-bc66105f1329",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyper Params\n",
    "batch_size = 1\n",
    "hidden_size = 120\n",
    "input_size = 120\n",
    "embed_dim = 120\n",
    "lr = 0.001\n",
    "epochs = 10\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0c6db8d-1973-46bf-9e46-5b790efd56b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = load_dataset(\"jaydenccc/AI_Storyteller_Dataset\", split=\"train\")['short_story']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b889e196-d6a1-4fca-9c89-6e3c753e337d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/grant/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "\n",
    "text_corpus = \"\"\n",
    "for item in raw_dataset:\n",
    "    text_corpus += item.lower()\n",
    "\n",
    "text_corpus = text_corpus.replace('.', '').replace(',','').replace('\\n',' ').replace('\\'', '').replace('\\\"', '').replace(':', '').replace('?', '').replace('!', '').replace(';', '').replace('-', '')\n",
    "\n",
    "tokens = word_tokenize(text_corpus)\n",
    "# tokens = [item for item in tokens if item not in ['the', '']]\n",
    "\n",
    "unique_words = set(tokens)\n",
    "word_to_num = {word: idx for idx, word in enumerate(unique_words)}\n",
    "num_to_word = {idx: word for idx, word in enumerate(unique_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b6fd7f9-58a7-47a5-9c6d-bbeee9b36698",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = len(word_to_num)\n",
    "\n",
    "pad_tok_id = output_size\n",
    "num_to_word[pad_tok_id] = '<pad>'\n",
    "word_to_num['<pad>'] = pad_tok_id\n",
    "\n",
    "num_to_word[output_size+1] = ''\n",
    "word_to_num[''] = output_size+1\n",
    "\n",
    "output_size = len(word_to_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "636a65fe-5b54-4f62-81a1-93a8744b7c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoryDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cabc52ba-4148-4020-a043-a7190e130852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pads = ['<pad>' for i in range(9)]\n",
    "corpus = text_corpus.split(' ')\n",
    "# corpus = [item for item in corpus if item not in ['the', '']]\n",
    "\n",
    "train_dataset = StoryDataset(corpus[:25000])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "test_dataset = StoryDataset(corpus[25000:])\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e595e7d8-5cf4-4c6c-9b93-3be26c249e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embed(x)\n",
    "        hidden = torch.zeros(1, 10, self.hidden_size).to(device)\n",
    "\n",
    "        # Forward through net\n",
    "        out, hidden = self.rnn(embedded, hidden)\n",
    "        # print(f\"RNN Out: {out.shape}\")\n",
    "        \n",
    "        # Only use last hidden state for output\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        # print(f\"FC Out: {out.shape}\")\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37fa4c9a-8494-4364-b780-9f815ee6f1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = RNNModel(embed_dim, output_size, hidden_size).to(device)\n",
    "crit = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d08c2ca9-ee85-42b5-8c52-c792937ef1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0866045093536377, Step: 0/25000, Epoch: 0/0\n",
      "Loss: 8.450979671478272, Step: 100/25000, Epoch: 0/0\n",
      "Loss: 8.135062236785888, Step: 200/25000, Epoch: 0/0\n",
      "Loss: 7.754678263664245, Step: 300/25000, Epoch: 0/0\n",
      "Loss: 7.792630143165589, Step: 400/25000, Epoch: 0/0\n",
      "Loss: 7.238177635669708, Step: 500/25000, Epoch: 0/0\n",
      "Loss: 7.262733550071716, Step: 600/25000, Epoch: 0/0\n",
      "Loss: 6.759184019565582, Step: 700/25000, Epoch: 0/0\n",
      "Loss: 7.087148329019547, Step: 800/25000, Epoch: 0/0\n",
      "Loss: 7.095429793596268, Step: 900/25000, Epoch: 0/0\n",
      "Loss: 6.692348681688308, Step: 1000/25000, Epoch: 0/0\n",
      "Loss: 7.578595131635666, Step: 1100/25000, Epoch: 0/0\n",
      "Loss: 6.867394018173218, Step: 1200/25000, Epoch: 0/0\n",
      "Loss: 7.193477474451065, Step: 1300/25000, Epoch: 0/0\n",
      "Loss: 7.087025964260102, Step: 1400/25000, Epoch: 0/0\n",
      "Loss: 6.379844310879707, Step: 1500/25000, Epoch: 0/0\n",
      "Loss: 6.6435691058635715, Step: 1600/25000, Epoch: 0/0\n",
      "Loss: 7.0360314673185345, Step: 1700/25000, Epoch: 0/0\n",
      "Loss: 6.813373584747314, Step: 1800/25000, Epoch: 0/0\n",
      "Loss: 7.140814464092255, Step: 1900/25000, Epoch: 0/0\n",
      "Loss: 6.731391507387161, Step: 2000/25000, Epoch: 0/0\n",
      "Loss: 6.217237365841865, Step: 2100/25000, Epoch: 0/0\n",
      "Loss: 6.58031583070755, Step: 2200/25000, Epoch: 0/0\n",
      "Loss: 7.221820142269134, Step: 2300/25000, Epoch: 0/0\n",
      "Loss: 6.9496861016750335, Step: 2400/25000, Epoch: 0/0\n",
      "Loss: 6.580153766274452, Step: 2500/25000, Epoch: 0/0\n",
      "Loss: 6.615168924331665, Step: 2600/25000, Epoch: 0/0\n",
      "Loss: 6.65629807651043, Step: 2700/25000, Epoch: 0/0\n",
      "Loss: 6.479015378952027, Step: 2800/25000, Epoch: 0/0\n",
      "Loss: 6.002449314594269, Step: 2900/25000, Epoch: 0/0\n",
      "Loss: 6.478857491016388, Step: 3000/25000, Epoch: 0/0\n",
      "Loss: 6.634237290620804, Step: 3100/25000, Epoch: 0/0\n",
      "Loss: 5.868789921104908, Step: 3200/25000, Epoch: 0/0\n",
      "Loss: 7.010000591278076, Step: 3300/25000, Epoch: 0/0\n",
      "Loss: 6.103106883764267, Step: 3400/25000, Epoch: 0/0\n",
      "Loss: 6.071822404265403, Step: 3500/25000, Epoch: 0/0\n",
      "Loss: 6.4527716016769405, Step: 3600/25000, Epoch: 0/0\n",
      "Loss: 6.698661198616028, Step: 3700/25000, Epoch: 0/0\n",
      "Loss: 6.715195198059082, Step: 3800/25000, Epoch: 0/0\n",
      "Loss: 6.08007036447525, Step: 3900/25000, Epoch: 0/0\n",
      "Loss: 6.11323453605175, Step: 4000/25000, Epoch: 0/0\n",
      "Loss: 6.618160501718521, Step: 4100/25000, Epoch: 0/0\n",
      "Loss: 6.337870270609856, Step: 4200/25000, Epoch: 0/0\n",
      "Loss: 6.180948524773121, Step: 4300/25000, Epoch: 0/0\n",
      "Loss: 5.874984960258007, Step: 4400/25000, Epoch: 0/0\n",
      "Loss: 6.932268096506595, Step: 4500/25000, Epoch: 0/0\n",
      "Loss: 6.807598324120045, Step: 4600/25000, Epoch: 0/0\n",
      "Loss: 6.877803490161896, Step: 4700/25000, Epoch: 0/0\n",
      "Loss: 6.519715021848679, Step: 4800/25000, Epoch: 0/0\n",
      "Loss: 6.474520929455757, Step: 4900/25000, Epoch: 0/0\n",
      "Loss: 5.814188545644283, Step: 5000/25000, Epoch: 0/0\n",
      "Loss: 5.595553870201111, Step: 5100/25000, Epoch: 0/0\n",
      "Loss: 6.32491517484188, Step: 5200/25000, Epoch: 0/0\n",
      "Loss: 7.165107111930848, Step: 5300/25000, Epoch: 0/0\n",
      "Loss: 5.985694112330675, Step: 5400/25000, Epoch: 0/0\n",
      "Loss: 5.963898068368435, Step: 5500/25000, Epoch: 0/0\n",
      "Loss: 6.34387504696846, Step: 5600/25000, Epoch: 0/0\n",
      "Loss: 6.797486696243286, Step: 5700/25000, Epoch: 0/0\n",
      "Loss: 7.2647782325744625, Step: 5800/25000, Epoch: 0/0\n",
      "Loss: 6.824676867723465, Step: 5900/25000, Epoch: 0/0\n",
      "Loss: 6.19363326728344, Step: 6000/25000, Epoch: 0/0\n",
      "Loss: 6.467129256725311, Step: 6100/25000, Epoch: 0/0\n",
      "Loss: 6.312484272420407, Step: 6200/25000, Epoch: 0/0\n",
      "Loss: 6.355295795798302, Step: 6300/25000, Epoch: 0/0\n",
      "Loss: 6.557960034310818, Step: 6400/25000, Epoch: 0/0\n",
      "Loss: 6.181321796178818, Step: 6500/25000, Epoch: 0/0\n",
      "Loss: 6.169433667361736, Step: 6600/25000, Epoch: 0/0\n",
      "Loss: 6.547492084354162, Step: 6700/25000, Epoch: 0/0\n",
      "Loss: 6.76608099102974, Step: 6800/25000, Epoch: 0/0\n",
      "Loss: 6.321317332983017, Step: 6900/25000, Epoch: 0/0\n",
      "Loss: 6.513081719875336, Step: 7000/25000, Epoch: 0/0\n",
      "Loss: 6.871391756534576, Step: 7100/25000, Epoch: 0/0\n",
      "Loss: 6.253706809282303, Step: 7200/25000, Epoch: 0/0\n",
      "Loss: 6.587155418395996, Step: 7300/25000, Epoch: 0/0\n",
      "Loss: 6.3772511279582975, Step: 7400/25000, Epoch: 0/0\n",
      "Loss: 6.885051123499871, Step: 7500/25000, Epoch: 0/0\n",
      "Loss: 6.080491608381271, Step: 7600/25000, Epoch: 0/0\n",
      "Loss: 6.472243545055389, Step: 7700/25000, Epoch: 0/0\n",
      "Loss: 6.378456881344318, Step: 7800/25000, Epoch: 0/0\n",
      "Loss: 5.729923311471939, Step: 7900/25000, Epoch: 0/0\n",
      "Loss: 5.539473447799683, Step: 8000/25000, Epoch: 0/0\n",
      "Loss: 6.659874665439129, Step: 8100/25000, Epoch: 0/0\n",
      "Loss: 6.270107288956642, Step: 8200/25000, Epoch: 0/0\n",
      "Loss: 5.884691162407398, Step: 8300/25000, Epoch: 0/0\n",
      "Loss: 6.444908359050751, Step: 8400/25000, Epoch: 0/0\n",
      "Loss: 6.806040427684784, Step: 8500/25000, Epoch: 0/0\n",
      "Loss: 6.611010598987341, Step: 8600/25000, Epoch: 0/0\n",
      "Loss: 6.818485150337219, Step: 8700/25000, Epoch: 0/0\n",
      "Loss: 6.0681769871711735, Step: 8800/25000, Epoch: 0/0\n",
      "Loss: 6.6388916629552845, Step: 8900/25000, Epoch: 0/0\n",
      "Loss: 6.288174895048141, Step: 9000/25000, Epoch: 0/0\n",
      "Loss: 6.789203876256943, Step: 9100/25000, Epoch: 0/0\n",
      "Loss: 6.30430156648159, Step: 9200/25000, Epoch: 0/0\n",
      "Loss: 6.8272768434882165, Step: 9300/25000, Epoch: 0/0\n",
      "Loss: 5.943981910943985, Step: 9400/25000, Epoch: 0/0\n",
      "Loss: 6.089603267610073, Step: 9500/25000, Epoch: 0/0\n",
      "Loss: 6.6622841539978985, Step: 9600/25000, Epoch: 0/0\n",
      "Loss: 6.533210521936416, Step: 9700/25000, Epoch: 0/0\n",
      "Loss: 5.315828234255314, Step: 9800/25000, Epoch: 0/0\n",
      "Loss: 6.09542000234127, Step: 9900/25000, Epoch: 0/0\n",
      "Loss: 6.214155423641205, Step: 10000/25000, Epoch: 0/0\n",
      "Loss: 6.155721858739853, Step: 10100/25000, Epoch: 0/0\n",
      "Loss: 6.287690191864967, Step: 10200/25000, Epoch: 0/0\n",
      "Loss: 6.444847331047058, Step: 10300/25000, Epoch: 0/0\n",
      "Loss: 6.032394211888313, Step: 10400/25000, Epoch: 0/0\n",
      "Loss: 5.47805310882628, Step: 10500/25000, Epoch: 0/0\n",
      "Loss: 7.149513322710991, Step: 10600/25000, Epoch: 0/0\n",
      "Loss: 7.2097896027565005, Step: 10700/25000, Epoch: 0/0\n",
      "Loss: 6.998572764396667, Step: 10800/25000, Epoch: 0/0\n",
      "Loss: 6.798902324438095, Step: 10900/25000, Epoch: 0/0\n",
      "Loss: 5.770598599612713, Step: 11000/25000, Epoch: 0/0\n",
      "Loss: 6.316882282495499, Step: 11100/25000, Epoch: 0/0\n",
      "Loss: 5.641977810263634, Step: 11200/25000, Epoch: 0/0\n",
      "Loss: 5.77023042500019, Step: 11300/25000, Epoch: 0/0\n",
      "Loss: 6.241669737100601, Step: 11400/25000, Epoch: 0/0\n",
      "Loss: 6.066417199373245, Step: 11500/25000, Epoch: 0/0\n",
      "Loss: 7.062390792369842, Step: 11600/25000, Epoch: 0/0\n",
      "Loss: 7.157762494981289, Step: 11700/25000, Epoch: 0/0\n",
      "Loss: 6.482872048020363, Step: 11800/25000, Epoch: 0/0\n",
      "Loss: 6.102313395291567, Step: 11900/25000, Epoch: 0/0\n",
      "Loss: 5.6562760078907015, Step: 12000/25000, Epoch: 0/0\n",
      "Loss: 6.10494726806879, Step: 12100/25000, Epoch: 0/0\n",
      "Loss: 6.750225802659989, Step: 12200/25000, Epoch: 0/0\n",
      "Loss: 6.424813901782036, Step: 12300/25000, Epoch: 0/0\n",
      "Loss: 5.7837119328975675, Step: 12400/25000, Epoch: 0/0\n",
      "Loss: 5.556400253772735, Step: 12500/25000, Epoch: 0/0\n",
      "Loss: 6.446446470618248, Step: 12600/25000, Epoch: 0/0\n",
      "Loss: 6.36290934085846, Step: 12700/25000, Epoch: 0/0\n",
      "Loss: 6.705364028215408, Step: 12800/25000, Epoch: 0/0\n",
      "Loss: 6.335486110150814, Step: 12900/25000, Epoch: 0/0\n",
      "Loss: 6.848526791036129, Step: 13000/25000, Epoch: 0/0\n",
      "Loss: 6.922754708528519, Step: 13100/25000, Epoch: 0/0\n",
      "Loss: 6.905537209510803, Step: 13200/25000, Epoch: 0/0\n",
      "Loss: 6.644183070063591, Step: 13300/25000, Epoch: 0/0\n",
      "Loss: 6.609824749827385, Step: 13400/25000, Epoch: 0/0\n",
      "Loss: 6.202362316846847, Step: 13500/25000, Epoch: 0/0\n",
      "Loss: 5.9181166765093804, Step: 13600/25000, Epoch: 0/0\n",
      "Loss: 6.455840311050415, Step: 13700/25000, Epoch: 0/0\n",
      "Loss: 5.871930864304304, Step: 13800/25000, Epoch: 0/0\n",
      "Loss: 5.988210456967354, Step: 13900/25000, Epoch: 0/0\n",
      "Loss: 6.590324673056602, Step: 14000/25000, Epoch: 0/0\n",
      "Loss: 6.044471874833107, Step: 14100/25000, Epoch: 0/0\n",
      "Loss: 6.657157599031925, Step: 14200/25000, Epoch: 0/0\n",
      "Loss: 6.433551821112633, Step: 14300/25000, Epoch: 0/0\n",
      "Loss: 6.519185763001442, Step: 14400/25000, Epoch: 0/0\n",
      "Loss: 7.035930424332618, Step: 14500/25000, Epoch: 0/0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     21\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 23\u001b[0m avg_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Step: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m(\u001b[38;5;241m25000\u001b[39m\u001b[38;5;241m*\u001b[39mepoch)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dataset)\u001b[38;5;241m*\u001b[39mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "avg_loss = 0\n",
    "net.train()\n",
    "for epoch in range(epochs):\n",
    "    for i in range(len(train_dataset)-10):\n",
    "        input_seq = [word_to_num[v] for v in train_dataset[i:i+10]]\n",
    "        target_tok = word_to_num[train_dataset[i+10]]\n",
    "        \n",
    "        input_tensor = torch.tensor(input_seq, dtype=torch.long).unsqueeze(0) # Shape [1, 10]\n",
    "        target_tensor = torch.tensor(target_tok, dtype=torch.long).unsqueeze(0) # Shape [1]\n",
    "\n",
    "        input_tensor, target_tensor = input_tensor.to(device), target_tensor.to(device)\n",
    "\n",
    "        # print(f\"Target tensor: {target_tensor}\")\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        output = net(input_tensor)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = crit(output, target_tensor)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        avg_loss += loss.item()\n",
    "\n",
    "        # if i % 100 == 0:\n",
    "        #     print(f'Loss: {avg_loss/100}, Step: {i+(25000*epoch)}/{len(train_dataset)*epochs}, Epoch: {epoch}/{epochs-1}')\n",
    "        #     avg_loss = 0\n",
    "    \n",
    "    print(f'Loss: {avg_loss/25000}, Epoch: {epoch+1}/{epochs}')\n",
    "    # torch.save(net.state_dict(), f\"rnn-nlp-epoch-{epoch}.pth\")\n",
    "    avg_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3ced6b-9c1d-410d-939c-fbe6e458dbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './next_word_2.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696243c6-5496-4dd4-a5bc-cbc10b11d25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 100\n",
    "seq = []\n",
    "cur = \"protecting\"\n",
    "\n",
    "for i in range(seq_len):\n",
    "    print(cur)\n",
    "    next_tok = net(torch.tensor([[word_to_num[cur]]], dtype=torch.long).to(device))\n",
    "    cur = num_to_word[torch.argmax(next_tok).item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e39294c-5fc7-49a8-b792-6c26a2c64f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = train_dataset[0:10]\n",
    "input_seq = [word_to_num[v] for v in seq]\n",
    "input_tensor = torch.tensor(input_seq, dtype=torch.long).unsqueeze(0).to(device)\n",
    "output = net(input_tensor)\n",
    "num_to_word[torch.argmax(output).item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd3c90a9-c246-4846-8e76-6cb2f7a45ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: the\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "input_tensor = torch.tensor([[word_to_num[\"the\"]]], dtype=torch.float32)\n",
    "\n",
    "# Forward pass\n",
    "output = torch.argmax(net(input_tensor))\n",
    "print(f'Word: {num_to_word[output.item()]}')\n",
    "\n",
    "plot = output.detach().numpy()\n",
    "plt.plot(plot)\n",
    "plt.show()\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.bar(range(len(fc1_logits[0])), fc1_logits[0], color='blue', alpha=0.7)\n",
    "# plt.title(\"Logits from fc1 Layer\")\n",
    "# plt.xlabel(\"Neuron Index\")\n",
    "# plt.ylabel(\"Logit Value\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "856c0b85-a19c-4b22-ba78-6abab9d9ac04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, output_size, embed_dim, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, 120),\n",
    "            nn.GELU(),\n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(120, 240),\n",
    "            nn.GELU(),\n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(240, output_size)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        embedded = self.embed(x)\n",
    "        embedded = embedded.view(embedded.size(0), -1)\n",
    "        return self.fc(embedded)\n",
    "        # return torch.argmax(x, dim=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
